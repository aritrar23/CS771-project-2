{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595ba2af-b0e5-4060-9580-834942719715",
   "metadata": {},
   "source": [
    "# Task 2 : QDA with updated means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cded09ad-8c9b-4d9f-a486-12a10343c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cefb628-f81e-4eb3-bcf3-f5db230ce6bb",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354ccad-4eab-40f7-85a4-5d321a25e40c",
   "metadata": {},
   "source": [
    "Here's our preprocessing pipeline - same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7d1f0f-e3e3-40c9-9e58-ca08c0168fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "    def __init__(self, n_components=50):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.transformer = PowerTransformer(method='yeo-johnson')\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.pca_mean = None\n",
    "        self.pca_std = None\n",
    "\n",
    "    def fit_transform(self, X_train):\n",
    "        \"\"\"\n",
    "        Fits the preprocessing pipeline on the training set and applies transformations.\n",
    "        \n",
    "        Parameters:\n",
    "        n: ndarray of shape (n_samples, n_features), training data.\n",
    "        \n",
    "        Returns:\n",
    "        - pca_train: Preprocessed training data.\n",
    "        \"\"\"\n",
    "        # Step 1: Standardization\n",
    "        standardized_train = self.scaler.fit_transform(X_train)\n",
    "\n",
    "        # Step 2: Yeo-Johnson Transformation\n",
    "        transformed_train = self.transformer.fit_transform(standardized_train)\n",
    "\n",
    "        # Step 3: PCA\n",
    "        pca_train = self.pca.fit_transform(transformed_train)\n",
    "\n",
    "        # Step 4: Standardize PCA-transformed data\n",
    "        self.pca_mean = np.mean(pca_train, axis=0)\n",
    "        self.pca_std = np.std(pca_train, axis=0)\n",
    "        pca_train_standardized = (pca_train - self.pca_mean) / self.pca_std\n",
    "\n",
    "        return pca_train_standardized\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies the fitted preprocessing pipeline to new data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray of shape (n_samples, n_features), data to preprocess.\n",
    "        \n",
    "        Returns:\n",
    "        - pca_transformed: Preprocessed data.\n",
    "        \"\"\"\n",
    "        # Step 1: Standardization\n",
    "        standardized = self.scaler.transform(X)\n",
    "\n",
    "        # Step 2: Yeo-Johnson Transformation\n",
    "        transformed = self.transformer.transform(standardized)\n",
    "\n",
    "        # Step 3: PCA\n",
    "        pca_transformed = self.pca.transform(transformed)\n",
    "\n",
    "        # Step 4: Standardize PCA-transformed data using training set stats\n",
    "        pca_transformed_standardized = (pca_transformed - self.pca_mean) / self.pca_std\n",
    "\n",
    "        return pca_transformed_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc901355-92e2-4735-afed-04d55065c8cc",
   "metadata": {},
   "source": [
    "We preprocess all 20 train and eval datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f6ad86-e88e-48fb-9df4-053107d3784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=PreprocessingPipeline()\n",
    "X_train=[pipeline.fit_transform(torch.load('extracted_data\\X_train_1.pth'))]+[pipeline.transform(torch.load(f'extracted_data\\X_train_{i}.pth')) for i in range(2,21)]\n",
    "X_eval=[pipeline.transform(torch.load(f'extracted_data\\X_eval_{i}.pth')) for i in range(1,21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e775d6-023e-4c2c-9a22-7526d47bf76d",
   "metadata": {},
   "source": [
    "## The modified QDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137e826-a417-4048-9b5b-5db97dde77e0",
   "metadata": {},
   "source": [
    "We have used the QDA model just like in the previous task. Here we have used the mod_update function instead of the normal update function. In mod_update, we first generate pseudolabels of the new dataset from a different distribution, then we estimate the parameters once again, now while updating we give 0.1 weightage to the new parameters and 0.9 weightage to the old parameters instead of using the number-of-examples weighted approach we did before. The value of 0.1 was chosen by manual hypertunning. This is done as otherwise the new datasets were getting really low weightage as the model size increased, so we set fixed weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404539d-3bc0-4944-897a-4e34b1421f2c",
   "metadata": {},
   "source": [
    "We had also tried 2 other approaches - i) Pseudodataset generation, data augmentation, followed by expectation maximization algorithm and ii) Minimizing the KL Divergence loss between the target domain (new dataset) and source domain (pseudodataset generated by trained model on previous datasets). They are described in the Task2_Experimentation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e17351-7955-4148-882d-1ac8ba625038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDAClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_means = {}\n",
    "        self.class_covariances = {}\n",
    "        self.class_priors = {}\n",
    "        self.class_counts = {}\n",
    "        self.total_samples = 0\n",
    "    \n",
    "    def fit(self, X, y): # For getting the means, covariances and class priors from our initial training data\n",
    "        \"\"\"\n",
    "        Fits the QDA model to the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray of shape (n_samples, n_features), training data\n",
    "        - y: ndarray of shape (n_samples,), class labels\n",
    "        \"\"\"\n",
    "        self.total_samples = X.shape[0]\n",
    "        classes = np.unique(y)\n",
    "        \n",
    "        for c in classes:\n",
    "            # Get data points belonging to class c\n",
    "            class_data = X[y == c]\n",
    "            class_count = class_data.shape[0]\n",
    "            \n",
    "            # Compute class-specific statistics\n",
    "            self.class_means[c] = np.mean(class_data, axis=0)\n",
    "            self.class_covariances[c] = np.cov(class_data, rowvar=False)\n",
    "            self.class_priors[c] = class_count / self.total_samples\n",
    "            self.class_counts[c] = class_count\n",
    "\n",
    "    def predict(self, X): #Predicting the class of test examples by computing posterior\n",
    "        \"\"\"\n",
    "        Predict the class labels for a dataset X.\n",
    "    \n",
    "        Parameters:\n",
    "        - X: ndarray of shape (n_samples, n_features), the input data matrix.\n",
    "    \n",
    "        Returns:\n",
    "        - predictions: ndarray of shape (n_samples,), the predicted class labels for each sample.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.zeros(n_samples, dtype=int)  # Initialize predictions array\n",
    "        \n",
    "        # Get the classes from the keys of class_means\n",
    "        classes = list(self.class_means.keys())\n",
    "    \n",
    "        for i in range(n_samples):\n",
    "            posteriors = []\n",
    "            for c in classes:\n",
    "                # Compute likelihood P(x | y = c) using the multivariate Gaussian PDF\n",
    "                mean = self.class_means[c]\n",
    "                cov = self.class_covariances[c]\n",
    "                prior = self.class_priors[c]\n",
    "                likelihood = multivariate_normal.pdf(X[i], mean=mean, cov=cov)\n",
    "    \n",
    "                # Compute posterior P(y = c | x) = P(x | y = c) * P(y = c)\n",
    "                posterior = likelihood * prior\n",
    "                posteriors.append(posterior)\n",
    "            \n",
    "            # Predict the class with the highest posterior\n",
    "            predictions[i] = classes[np.argmax(posteriors)]\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_class_statistics(self): #Getting the current stored mean, covariance and prior for each class\n",
    "        \"\"\"\n",
    "        Returns the learned statistics for each class.\n",
    "        \n",
    "        Returns:\n",
    "        - class_means: dict of class means\n",
    "        - class_covariances: dict of class covariance matrices\n",
    "        - class_priors: dict of class priors\n",
    "        - class_counts: dict of the number of samples per class\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'means': self.class_means,\n",
    "            'covariances': self.class_covariances,\n",
    "            'priors': self.class_priors,\n",
    "            'counts': self.class_counts,\n",
    "        }\n",
    "    def update(self, X): # For updating model f_i to f_i+1, by first predicting labels of unlabelled dataset, then training on this new dataset, and\n",
    "                         # finally updating the model parameters by weighted average - taking into account the no. of training examples seen till now  \n",
    "                         # and the no. of new test examples.\n",
    "\n",
    "    # Step 2: Predict labels\n",
    "        predicted_labels = self.predict(X)\n",
    "    \n",
    "    # Step 3: Update statistics per class\n",
    "        for c in self.class_means.keys():\n",
    "            # Get new samples for class c\n",
    "            new_samples = X[predicted_labels == c]\n",
    "            new_count = new_samples.shape[0]\n",
    "            if new_count == 0:\n",
    "                continue  # No new samples for this class\n",
    "        \n",
    "            # Update mean\n",
    "            current_mean = self.class_means[c]\n",
    "            current_count = self.class_counts[c]\n",
    "            new_mean = (current_mean * current_count + new_samples.sum(axis=0)) / (current_count + new_count)\n",
    "        \n",
    "            # Update covariance\n",
    "            current_cov = self.class_covariances[c]\n",
    "            scatter_current = current_cov * current_count\n",
    "            scatter_new = np.cov(new_samples, rowvar=False) * new_count\n",
    "            scatter_updated = scatter_current + scatter_new\n",
    "            updated_cov = scatter_updated / (current_count + new_count)\n",
    "        \n",
    "        # Update priors, counts, etc.\n",
    "            self.class_means[c] = new_mean\n",
    "            self.class_covariances[c] = updated_cov\n",
    "            self.class_counts[c] += new_count\n",
    "            self.class_priors[c] = self.class_counts[c] / self.total_samples\n",
    "\n",
    "    # Update total sample count\n",
    "        self.total_samples += X.shape[0]\n",
    "    def mod_update(self, X, h):  # For updating model f_i to f_i+1, by first predicting labels of unlabelled dataset, then training on this new dataset, and\n",
    "                                 # finally updating the model parameters by weighted average - taking into account the hyperparameter h.\n",
    "        predicted_labels = self.predict(X)\n",
    "        for c in self.class_means.keys():\n",
    "            # Get new samples for class c\n",
    "            new_samples = X[predicted_labels == c]\n",
    "            new_count = new_samples.shape[0]\n",
    "            if new_count == 0:\n",
    "                continue  # No new samples for this class\n",
    "        \n",
    "            # Update mean\n",
    "            current_mean = self.class_means[c]\n",
    "            # current_count = self.class_counts[c]\n",
    "            new_mean = current_mean *(1-h) + (h)*new_samples.sum(axis=0)/new_count\n",
    "        \n",
    "            # Update covariance\n",
    "            current_cov = self.class_covariances[c]\n",
    "            scatter_current = current_cov * (1-h)\n",
    "            scatter_new = np.cov(new_samples, rowvar=False) * h\n",
    "            scatter_updated = scatter_current + scatter_new\n",
    "            updated_cov = scatter_updated\n",
    "        \n",
    "        # Update priors, counts, etc.\n",
    "            self.class_means[c] = new_mean\n",
    "            self.class_covariances[c] = updated_cov\n",
    "            self.class_priors[c] = self.class_counts[c]*(1-h) + new_count*h\n",
    "            self.class_counts[c] += new_count\n",
    "    \n",
    "    \n",
    "    def generate_samples(self, num_samples): #Not used in this approach, explained in Experimentation\n",
    "        \"\"\"\n",
    "        Generate synthetic samples using the learned class distributions.\n",
    "        \n",
    "        Parameters:\n",
    "        - num_samples: int, total number of synthetic samples to generate.\n",
    "        \n",
    "        Returns:\n",
    "        - X_generated: ndarray of shape (num_samples, n_features), the generated samples.\n",
    "        - y_generated: ndarray of shape (num_samples,), the corresponding class labels.\n",
    "        \"\"\"\n",
    "        # Initialize storage for generated samples and labels\n",
    "        X_generated = []\n",
    "        y_generated = []\n",
    "\n",
    "        # Generate samples for each class based on the prior probabilities\n",
    "        for c, prior in self.class_priors.items():\n",
    "            # Number of samples to generate for this class\n",
    "            class_samples = int(np.round(prior * num_samples))\n",
    "            \n",
    "            # Sample from the Gaussian distribution for this class\n",
    "            mean = self.class_means[c]\n",
    "            cov = self.class_covariances[c]\n",
    "            generated = np.random.multivariate_normal(mean, cov, size=class_samples)\n",
    "            \n",
    "            # Append to the result\n",
    "            X_generated.append(generated)\n",
    "            y_generated.extend([c] * class_samples)\n",
    "\n",
    "        # Concatenate and shuffle to create the final dataset\n",
    "        X_generated = np.vstack(X_generated)\n",
    "        y_generated = np.array(y_generated)\n",
    "        indices = np.arange(len(y_generated))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        return X_generated[indices], y_generated[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e33b3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 11 set\n",
      "done 12 set\n",
      "done 13 set\n",
      "done 14 set\n",
      "done 15 set\n",
      "done 16 set\n",
      "done 17 set\n",
      "done 18 set\n",
      "done 19 set\n",
      "done 20 set\n",
      "done model 1, set 1 \n",
      "done model 1, set 2 \n",
      "done model 1, set 3 \n",
      "done model 1, set 4 \n",
      "done model 1, set 5 \n",
      "done model 1, set 6 \n",
      "done model 1, set 7 \n",
      "done model 1, set 8 \n",
      "done model 1, set 9 \n",
      "done model 1, set 10 \n",
      "done model 1, set 11 \n",
      "done model 2, set 1 \n",
      "done model 2, set 2 \n",
      "done model 2, set 3 \n",
      "done model 2, set 4 \n",
      "done model 2, set 5 \n",
      "done model 2, set 6 \n",
      "done model 2, set 7 \n",
      "done model 2, set 8 \n",
      "done model 2, set 9 \n",
      "done model 2, set 10 \n",
      "done model 2, set 11 \n",
      "done model 2, set 12 \n",
      "done model 3, set 1 \n",
      "done model 3, set 2 \n",
      "done model 3, set 3 \n",
      "done model 3, set 4 \n",
      "done model 3, set 5 \n",
      "done model 3, set 6 \n",
      "done model 3, set 7 \n",
      "done model 3, set 8 \n",
      "done model 3, set 9 \n",
      "done model 3, set 10 \n",
      "done model 3, set 11 \n",
      "done model 3, set 12 \n",
      "done model 3, set 13 \n",
      "done model 4, set 1 \n",
      "done model 4, set 2 \n",
      "done model 4, set 3 \n",
      "done model 4, set 4 \n",
      "done model 4, set 5 \n",
      "done model 4, set 6 \n",
      "done model 4, set 7 \n",
      "done model 4, set 8 \n",
      "done model 4, set 9 \n",
      "done model 4, set 10 \n",
      "done model 4, set 11 \n",
      "done model 4, set 12 \n",
      "done model 4, set 13 \n",
      "done model 4, set 14 \n",
      "done model 5, set 1 \n",
      "done model 5, set 2 \n",
      "done model 5, set 3 \n",
      "done model 5, set 4 \n",
      "done model 5, set 5 \n",
      "done model 5, set 6 \n",
      "done model 5, set 7 \n",
      "done model 5, set 8 \n",
      "done model 5, set 9 \n",
      "done model 5, set 10 \n",
      "done model 5, set 11 \n",
      "done model 5, set 12 \n",
      "done model 5, set 13 \n",
      "done model 5, set 14 \n",
      "done model 5, set 15 \n",
      "done model 6, set 1 \n",
      "done model 6, set 2 \n",
      "done model 6, set 3 \n",
      "done model 6, set 4 \n",
      "done model 6, set 5 \n",
      "done model 6, set 6 \n",
      "done model 6, set 7 \n",
      "done model 6, set 8 \n",
      "done model 6, set 9 \n",
      "done model 6, set 10 \n",
      "done model 6, set 11 \n",
      "done model 6, set 12 \n",
      "done model 6, set 13 \n",
      "done model 6, set 14 \n",
      "done model 6, set 15 \n",
      "done model 6, set 16 \n",
      "done model 7, set 1 \n",
      "done model 7, set 2 \n",
      "done model 7, set 3 \n",
      "done model 7, set 4 \n",
      "done model 7, set 5 \n",
      "done model 7, set 6 \n",
      "done model 7, set 7 \n",
      "done model 7, set 8 \n",
      "done model 7, set 9 \n",
      "done model 7, set 10 \n",
      "done model 7, set 11 \n",
      "done model 7, set 12 \n",
      "done model 7, set 13 \n",
      "done model 7, set 14 \n",
      "done model 7, set 15 \n",
      "done model 7, set 16 \n",
      "done model 7, set 17 \n",
      "done model 8, set 1 \n",
      "done model 8, set 2 \n",
      "done model 8, set 3 \n",
      "done model 8, set 4 \n",
      "done model 8, set 5 \n",
      "done model 8, set 6 \n",
      "done model 8, set 7 \n",
      "done model 8, set 8 \n",
      "done model 8, set 9 \n",
      "done model 8, set 10 \n",
      "done model 8, set 11 \n",
      "done model 8, set 12 \n",
      "done model 8, set 13 \n",
      "done model 8, set 14 \n",
      "done model 8, set 15 \n",
      "done model 8, set 16 \n",
      "done model 8, set 17 \n",
      "done model 8, set 18 \n",
      "done model 9, set 1 \n",
      "done model 9, set 2 \n",
      "done model 9, set 3 \n",
      "done model 9, set 4 \n",
      "done model 9, set 5 \n",
      "done model 9, set 6 \n",
      "done model 9, set 7 \n",
      "done model 9, set 8 \n",
      "done model 9, set 9 \n",
      "done model 9, set 10 \n",
      "done model 9, set 11 \n",
      "done model 9, set 12 \n",
      "done model 9, set 13 \n",
      "done model 9, set 14 \n",
      "done model 9, set 15 \n",
      "done model 9, set 16 \n",
      "done model 9, set 17 \n",
      "done model 9, set 18 \n",
      "done model 9, set 19 \n",
      "done model 10, set 1 \n",
      "done model 10, set 2 \n",
      "done model 10, set 3 \n",
      "done model 10, set 4 \n",
      "done model 10, set 5 \n",
      "done model 10, set 6 \n",
      "done model 10, set 7 \n",
      "done model 10, set 8 \n",
      "done model 10, set 9 \n",
      "done model 10, set 10 \n",
      "done model 10, set 11 \n",
      "done model 10, set 12 \n",
      "done model 10, set 13 \n",
      "done model 10, set 14 \n",
      "done model 10, set 15 \n",
      "done model 10, set 16 \n",
      "done model 10, set 17 \n",
      "done model 10, set 18 \n",
      "done model 10, set 19 \n",
      "done model 10, set 20 \n",
      "[[82.36 83.8  82.48 82.76 83.   83.96 82.6  83.4  82.76 83.4  60.84  0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [82.08 83.56 82.48 82.68 82.88 83.76 82.56 83.48 82.4  83.52 60.44 38.2\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [81.48 83.56 82.08 82.16 82.68 83.64 82.28 83.12 82.2  83.2  60.   37.8\n",
      "  68.96  0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [81.28 83.24 82.08 82.2  82.36 83.48 82.08 82.72 82.24 82.92 59.76 37.72\n",
      "  69.   79.48  0.    0.    0.    0.    0.    0.  ]\n",
      " [81.2  83.08 81.76 82.   82.32 83.16 81.92 82.72 82.   82.6  59.36 37.44\n",
      "  68.64 79.4  81.6   0.    0.    0.    0.    0.  ]\n",
      " [81.08 82.96 81.4  81.88 82.16 83.12 81.64 82.28 81.6  82.4  58.96 37.08\n",
      "  68.24 79.2  81.52 61.4   0.    0.    0.    0.  ]\n",
      " [80.88 82.88 81.4  81.48 82.2  83.   81.24 82.   81.76 82.16 58.76 36.92\n",
      "  68.12 79.04 81.4  61.4  70.72  0.    0.    0.  ]\n",
      " [80.68 82.72 81.28 81.72 82.16 82.84 81.24 81.8  81.8  82.12 58.68 37.\n",
      "  68.   79.04 81.24 61.44 70.64 66.32  0.    0.  ]\n",
      " [80.28 82.52 81.2  81.32 81.96 82.84 80.92 81.44 81.64 82.08 58.24 37.2\n",
      "  67.72 78.76 80.92 61.4  70.4  66.2  62.8   0.  ]\n",
      " [80.16 82.36 81.08 81.16 81.76 82.6  80.76 81.08 81.28 81.88 57.96 36.8\n",
      "  67.08 78.2  80.72 61.08 70.04 65.56 62.72 74.8 ]]\n"
     ]
    }
   ],
   "source": [
    "qdm=pickle.load(open(\"final_f10.pkl\", \"rb\")) #Loading our saved model\n",
    "for i in range(10,20):\n",
    "    qdm.mod_update(X_train[i], 0.1)\n",
    "    pickle.dump(qdm, open(f\"f{i+1}.pkl\", \"wb\")) #Saving the new models\n",
    "    print(f\"done {i+1} set\")\n",
    "\n",
    "model_list=[pickle.load(open(f'f{i+1}.pkl','rb')) for i in range(10,20)]\n",
    "\n",
    "#Evaluating the models\n",
    "accuracies=np.zeros((10,20))\n",
    "for i in range(10):\n",
    "    for j in range(i+11):\n",
    "        prediction=model_list[i].predict(X_eval[j])\n",
    "        accuracies[i][j]=np.mean(prediction == y_eval[j])*100\n",
    "        print(f\"done model {i+1}, set {j+1} \")\n",
    "\n",
    "print(accuracies)\n",
    "with open('accuracies.pkl', 'wb') as f:\n",
    "    pickle.dump(accuracies, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81444464-3a55-4981-b121-a2f5d4f06286",
   "metadata": {},
   "source": [
    "These are the final accuracies obtained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
