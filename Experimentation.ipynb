{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac06828-d84f-448f-aa4d-661ccf78c470",
   "metadata": {},
   "source": [
    "# Experimentation - Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d50f5d-b6b4-4e88-8903-1d7dc061b46c",
   "metadata": {},
   "source": [
    "We tried mainly 3 other approaches, which are described here - \n",
    "\n",
    "i) Pseudo-dataset generation and then applying Gaussian Mixture Model using Expectation Maximization\n",
    "\n",
    "ii)\tKL-Divergence minimization between new datasets and pseudo-generated dataset\n",
    "\n",
    "iii) Loss function minimization (as described in Paper 2 - https://arxiv.org/pdf/2301.10418)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae40994-e9eb-4c54-b742-72e0b7f58d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc16e0-dc7a-44a8-8d04-a75467ec0a7b",
   "metadata": {},
   "source": [
    "## 1) GMM using EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211915dc-a211-4adb-91d3-5afd1477d9eb",
   "metadata": {},
   "source": [
    "The preprocessing steps remain the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80192d91-089f-41fa-856f-656f29956c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "    def __init__(self, n_components=50):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.transformer = PowerTransformer(method='yeo-johnson')\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.pca_mean = None\n",
    "        self.pca_std = None\n",
    "\n",
    "    def fit_transform(self, X_train):\n",
    "        \"\"\"\n",
    "        Fits the preprocessing pipeline on the training set and applies transformations.\n",
    "        \n",
    "        Parameters:\n",
    "        n: ndarray of shape (n_samples, n_features), training data.\n",
    "        \n",
    "        Returns:\n",
    "        - pca_train: Preprocessed training data.\n",
    "        \"\"\"\n",
    "        # Step 1: Standardization\n",
    "        standardized_train = self.scaler.fit_transform(X_train)\n",
    "\n",
    "        # Step 2: Yeo-Johnson Transformation\n",
    "        transformed_train = self.transformer.fit_transform(standardized_train)\n",
    "\n",
    "        # Step 3: PCA\n",
    "        pca_train = self.pca.fit_transform(transformed_train)\n",
    "\n",
    "        # Step 4: Standardize PCA-transformed data\n",
    "        self.pca_mean = np.mean(pca_train, axis=0)\n",
    "        self.pca_std = np.std(pca_train, axis=0)\n",
    "        pca_train_standardized = (pca_train - self.pca_mean) / self.pca_std\n",
    "\n",
    "        return pca_train_standardized\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies the fitted preprocessing pipeline to new data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray of shape (n_samples, n_features), data to preprocess.\n",
    "        \n",
    "        Returns:\n",
    "        - pca_transformed: Preprocessed data.\n",
    "        \"\"\"\n",
    "        # Step 1: Standardization\n",
    "        standardized = self.scaler.transform(X)\n",
    "\n",
    "        # Step 2: Yeo-Johnson Transformation\n",
    "        transformed = self.transformer.transform(standardized)\n",
    "\n",
    "        # Step 3: PCA\n",
    "        pca_transformed = self.pca.transform(transformed)\n",
    "\n",
    "        # Step 4: Standardize PCA-transformed data using training set stats\n",
    "        pca_transformed_standardized = (pca_transformed - self.pca_mean) / self.pca_std\n",
    "\n",
    "        return pca_transformed_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f17877-890d-4fba-b85f-2ce3cab34db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=PreprocessingPipeline()\n",
    "X_train=[pipeline.fit_transform(torch.load('extracted_data\\X_train_1.pth'))]+[pipeline.transform(torch.load(f'extracted_data\\X_train_{i}.pth')) for i in range(2,21)]\n",
    "X_eval=[pipeline.transform(torch.load(f'extracted_data\\X_eval_{i}.pth')) for i in range(1,21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831b74b-e7b0-4221-a0b7-8e3e2d6a4e12",
   "metadata": {},
   "source": [
    "This is the modified QDA model including pseudo-dataset generation and expectation maximization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b35e474-ef8c-4491-ab23-4c34e79a28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class QDAClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_means = {}\n",
    "        self.class_covariances = {}\n",
    "        self.class_priors = {}\n",
    "        self.class_counts = {}\n",
    "        self.total_samples = 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the QDA model to the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray of shape (n_samples, n_features), training data\n",
    "        - y: ndarray of shape (n_samples,), class labels\n",
    "        \"\"\"\n",
    "        self.total_samples = X.shape[0]\n",
    "        classes = np.unique(y)\n",
    "        \n",
    "        for c in classes:\n",
    "            # Get data points belonging to class c\n",
    "            class_data = X[y == c]\n",
    "            class_count = class_data.shape[0]\n",
    "            \n",
    "            # Compute class-specific statistics\n",
    "            self.class_means[c] = np.mean(class_data, axis=0)\n",
    "            self.class_covariances[c] = np.cov(class_data, rowvar=False)\n",
    "            self.class_priors[c] = class_count / self.total_samples\n",
    "            self.class_counts[c] = class_count\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for a dataset X.\n",
    "    \n",
    "        Parameters:\n",
    "        - X: ndarray of shape (n_samples, n_features), the input data matrix.\n",
    "    \n",
    "        Returns:\n",
    "        - predictions: ndarray of shape (n_samples,), the predicted class labels for each sample.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.zeros(n_samples, dtype=int)  # Initialize predictions array\n",
    "        \n",
    "        # Get the classes from the keys of class_means\n",
    "        classes = list(self.class_means.keys())\n",
    "    \n",
    "        for i in range(n_samples):\n",
    "            posteriors = []\n",
    "            for c in classes:\n",
    "                # Compute likelihood P(x | y = c) using the multivariate Gaussian PDF\n",
    "                mean = self.class_means[c]\n",
    "                cov = self.class_covariances[c]\n",
    "                prior = self.class_priors[c]\n",
    "                likelihood = multivariate_normal.pdf(X[i], mean=mean, cov=cov)\n",
    "    \n",
    "                # Compute posterior P(y = c | x) = P(x | y = c) * P(y = c)\n",
    "                posterior = likelihood * prior\n",
    "                posteriors.append(posterior)\n",
    "            \n",
    "            # Predict the class with the highest posterior\n",
    "            predictions[i] = classes[np.argmax(posteriors)]\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_class_statistics(self):\n",
    "        \"\"\"\n",
    "        Returns the learned statistics for each class.\n",
    "        \n",
    "        Returns:\n",
    "        - class_means: dict of class means\n",
    "        - class_covariances: dict of class covariance matrices\n",
    "        - class_priors: dict of class priors\n",
    "        - class_counts: dict of the number of samples per class\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'means': self.class_means,\n",
    "            'covariances': self.class_covariances,\n",
    "            'priors': self.class_priors,\n",
    "            'counts': self.class_counts,\n",
    "        }\n",
    "    def update(self, X):\n",
    "\n",
    "    # Step 2: Predict labels\n",
    "        predicted_labels = self.predict(X)\n",
    "    \n",
    "    # Step 3: Update statistics per class\n",
    "        for c in self.class_means.keys():\n",
    "            # Get new samples for class c\n",
    "            new_samples = X[predicted_labels == c]\n",
    "            new_count = new_samples.shape[0]\n",
    "            if new_count == 0:\n",
    "                continue  # No new samples for this class\n",
    "        \n",
    "            # Update mean\n",
    "            current_mean = self.class_means[c]\n",
    "            current_count = self.class_counts[c]\n",
    "            new_mean = (current_mean * current_count + new_samples.sum(axis=0)) / (current_count + new_count)\n",
    "        \n",
    "            # Update covariance\n",
    "            current_cov = self.class_covariances[c]\n",
    "            scatter_current = current_cov * current_count\n",
    "            scatter_new = np.cov(new_samples, rowvar=False) * new_count\n",
    "            scatter_updated = scatter_current + scatter_new\n",
    "            updated_cov = scatter_updated / (current_count + new_count)\n",
    "        \n",
    "        # Update priors, counts, etc.\n",
    "            self.class_means[c] = new_mean\n",
    "            self.class_covariances[c] = updated_cov\n",
    "            self.class_counts[c] += new_count\n",
    "            self.class_priors[c] = self.class_counts[c] / self.total_samples\n",
    "\n",
    "    # Update total sample count\n",
    "        self.total_samples += X.shape[0]\n",
    "    def mod_update(self, X, h):\n",
    "        predicted_labels = self.predict(X)\n",
    "        for c in self.class_means.keys():\n",
    "            # Get new samples for class c\n",
    "            new_samples = X[predicted_labels == c]\n",
    "            new_count = new_samples.shape[0]\n",
    "            if new_count == 0:\n",
    "                continue  # No new samples for this class\n",
    "        \n",
    "            # Update mean\n",
    "            current_mean = self.class_means[c]\n",
    "            # current_count = self.class_counts[c]\n",
    "            new_mean = current_mean *(1-h) + (h)*new_samples.sum(axis=0)/new_count\n",
    "        \n",
    "            # Update covariance\n",
    "            current_cov = self.class_covariances[c]\n",
    "            scatter_current = current_cov * (1-h)\n",
    "            scatter_new = np.cov(new_samples, rowvar=False) * h\n",
    "            scatter_updated = scatter_current + scatter_new\n",
    "            updated_cov = scatter_updated\n",
    "        \n",
    "        # Update priors, counts, etc.\n",
    "            self.class_means[c] = new_mean\n",
    "            self.class_covariances[c] = updated_cov\n",
    "            self.class_priors[c] = self.class_counts[c]*(1-h) + new_count*h\n",
    "            self.class_counts[c] += new_count\n",
    "    \n",
    "    \n",
    "    def generate_samples(self, num_samples):\n",
    "        \"\"\"\n",
    "        Generate synthetic samples using the learned class distributions.\n",
    "        \n",
    "        Parameters:\n",
    "        - num_samples: int, total number of synthetic samples to generate.\n",
    "        \n",
    "        Returns:\n",
    "        - X_generated: ndarray of shape (num_samples, n_features), the generated samples.\n",
    "        - y_generated: ndarray of shape (num_samples,), the corresponding class labels.\n",
    "        \"\"\"\n",
    "        # Initialize storage for generated samples and labels\n",
    "        X_generated = []\n",
    "        y_generated = []\n",
    "\n",
    "        # Generate samples for each class based on the prior probabilities\n",
    "        for c, prior in self.class_priors.items():\n",
    "            # Number of samples to generate for this class\n",
    "            class_samples = int(np.round(prior * num_samples))\n",
    "            \n",
    "            # Sample from the Gaussian distribution for this class\n",
    "            mean = self.class_means[c]\n",
    "            cov = self.class_covariances[c]\n",
    "            generated = np.random.multivariate_normal(mean, cov, size=class_samples)\n",
    "            \n",
    "            # Append to the result\n",
    "            X_generated.append(generated)\n",
    "            y_generated.extend([c] * class_samples)\n",
    "\n",
    "        # Concatenate and shuffle to create the final dataset\n",
    "        X_generated = np.vstack(X_generated)\n",
    "        y_generated = np.array(y_generated)\n",
    "        indices = np.arange(len(y_generated))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        return X_generated[indices], y_generated[indices]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class QDAClassifierWithEM(QDAClassifier):\n",
    "    def expectation_maximization(self, X, max_iter=100, tol=1e-6, regularization=1e-6, min_iter=10):\n",
    "        \"\"\"\n",
    "        Performs the EM algorithm to refine the QDA parameters using unlabeled data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray of shape (n_samples, n_features), unlabeled data\n",
    "        - max_iter: int, maximum number of EM iterations\n",
    "        - tol: float, convergence tolerance for parameter changes\n",
    "        - regularization: float, regularization term for covariance matrices\n",
    "        - min_iter: int, minimum number of iterations to avoid premature stopping\n",
    "        \"\"\"\n",
    "        X_generated=self.generate_samples(len(X))\n",
    "        X=np.vstack((X_generated, X))\n",
    "        # Initialize parameters from the current QDA model\n",
    "        classes = list(self.class_means.keys())\n",
    "        k = len(classes)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize variables\n",
    "        responsibilities = np.zeros((n_samples, k))\n",
    "        log_likelihood = -np.inf\n",
    "        temp_class_means=self.class_means\n",
    "        temp_class_counts=self.class_counts\n",
    "        temp_class_covariances=self.class_covariances\n",
    "        temp_class_priors=self.class_priors\n",
    "        for iteration in range(max_iter):\n",
    "            # Step 1: Expectation (E-step)\n",
    "            log_probabilities = np.zeros((n_samples, k))\n",
    "            for i, c in enumerate(classes):\n",
    "                mean = temp_class_means[c]\n",
    "                cov = temp_class_covariances[c] + regularization * np.eye(n_features)  # Regularized covariance\n",
    "                prior = temp_class_priors[c]\n",
    "                log_probabilities[:, i] = np.log(prior) + multivariate_normal.logpdf(X, mean=mean, cov=cov)\n",
    "\n",
    "            # Stabilized log-sum-exp for log-likelihood normalization\n",
    "            log_responsibilities = log_probabilities - logsumexp(log_probabilities, axis=1, keepdims=True)\n",
    "            responsibilities = np.exp(log_responsibilities)\n",
    "\n",
    "            # Step 2: Maximization (M-step)\n",
    "            new_means, new_covariances, new_priors, new_class_counts = {}, {}, {}, {}\n",
    "            for i, c in enumerate(classes):\n",
    "                effective_count = responsibilities[:, i].sum()\n",
    "                new_class_counts[c] = effective_count\n",
    "\n",
    "                # Update mean\n",
    "                new_mean = (responsibilities[:, i][:, np.newaxis] * X).sum(axis=0) / effective_count\n",
    "                new_means[c] = new_mean\n",
    "\n",
    "                # Update covariance\n",
    "                centered_X = X - new_mean\n",
    "                weighted_cov = (responsibilities[:, i][:, np.newaxis, np.newaxis] * \n",
    "                                np.einsum('ni,nj->nij', centered_X, centered_X)).sum(axis=0)\n",
    "                new_covariances[c] = weighted_cov / effective_count + regularization * np.eye(n_features)\n",
    "\n",
    "                # Update prior\n",
    "                new_priors[c] = effective_count / n_samples\n",
    "\n",
    "            # Compute log-likelihood\n",
    "            new_log_likelihood = logsumexp(log_probabilities).sum()\n",
    "\n",
    "            # Convergence check\n",
    "            if iteration >= min_iter and np.abs(new_log_likelihood - log_likelihood) < tol:\n",
    "                print(f\"EM converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "\n",
    "            log_likelihood = new_log_likelihood\n",
    "     \n",
    "            # Update parameters for the next iteration\n",
    "            temp_class_means = new_means\n",
    "            temp_class_covariances = new_covariances\n",
    "            temp_class_priors = new_priors\n",
    "            temp_class_counts = new_class_counts\n",
    "        \n",
    "        for c in classes:\n",
    "            #Combine counts\n",
    "            total_count = self.class_counts[c] + new_class_counts[c]\n",
    "\n",
    "            # Weighted mean update\n",
    "            self.class_means[c] = (self.class_means[c] * self.class_counts[c] + new_means[c] * new_class_counts[c]) / total_count\n",
    "\n",
    "            # Weighted covariance update\n",
    "            centered_self_cov = self.class_covariances[c] * self.class_counts[c]\n",
    "            centered_new_cov = new_covariances[c] * new_class_counts[c]\n",
    "    \n",
    "            scatter_combined = (centered_self_cov + centered_new_cov + (self.class_counts[c] * new_class_counts[c]) / total_count * np.outer(self.class_means[c] - new_means[c], self.class_means[c] - new_means[c]))\n",
    "            self.class_covariances[c] = scatter_combined / total_count\n",
    "\n",
    "                # Update prior probabilities\n",
    "            self.class_priors[c] = total_count/ (self.total_samples + n_samples)\n",
    "\n",
    "                # Update total counts\n",
    "            self.class_counts[c] = total_count\n",
    "\n",
    "        self.total_samples=self.total_samples+n_samples\n",
    "\n",
    "        print(f\"EM completed in {iteration + 1} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d989677-ea3c-49f0-8f11-9f3afbfbb2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\ARITRA\\Documents\\Notebooks\\CS771_MiniProject2\\dataset\\dataset\\part_one_dataset\"\n",
    "traindata_1 = torch.load(f\"{path}\\\\train_data\\\\1_train_data.tar.pth\", map_location=torch.device('cpu'))\n",
    "y_train_1 = traindata_1['targets']\n",
    "qdaem = QDAClassifierWithEM()\n",
    "qdaem.fit(X_train[0], y_train_1)\n",
    "for i in range(1,10):\n",
    "    qdaem.update(X_train[i])\n",
    "    print(\"Done\")\n",
    "pickle.dump(qdaem, open(\"final_f10.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fcbe9-45d5-46b2-9842-650445f48578",
   "metadata": {},
   "source": [
    "## 2) KL-Divergence minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724e291a-a9d1-4677-b85c-078827d0e48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "Transformed Mean (mu_k): [0.37454019 0.95071425 0.73199401]\n",
      "Transformed Covariance (cov_k):\n",
      "[[0.99999974 0.         0.        ]\n",
      " [0.         1.00000011 0.        ]\n",
      " [0.         0.         1.00000022]]\n",
      "\n",
      "Class 2:\n",
      "Transformed Mean (mu_k): [0.59865906 0.15601888 0.15599435]\n",
      "Transformed Covariance (cov_k):\n",
      "[[1.0000004  0.         0.        ]\n",
      " [0.         1.00000001 0.        ]\n",
      " [0.         0.         1.00000037]]\n",
      "\n",
      "Class 3:\n",
      "Transformed Mean (mu_k): [0.05808353 0.86617708 0.60111572]\n",
      "Transformed Covariance (cov_k):\n",
      "[[1.00000039 0.         0.        ]\n",
      " [0.         1.00000059 0.        ]\n",
      " [0.         0.         1.00000037]]\n",
      "\n",
      "Class 4:\n",
      "Transformed Mean (mu_k): [0.7080738  0.02058475 0.96990983]\n",
      "Transformed Covariance (cov_k):\n",
      "[[1.00000417 0.         0.        ]\n",
      " [0.         0.99999926 0.        ]\n",
      " [0.         0.         1.00000582]]\n",
      "\n",
      "Class 5:\n",
      "Transformed Mean (mu_k): [0.8324418  0.21233873 0.18182489]\n",
      "Transformed Covariance (cov_k):\n",
      "[[0.99999839 0.         0.        ]\n",
      " [0.         1.00000255 0.        ]\n",
      " [0.         0.         0.99999842]]\n",
      "\n",
      "Class 6:\n",
      "Transformed Mean (mu_k): [0.18340418 0.30424234 0.52475692]\n",
      "Transformed Covariance (cov_k):\n",
      "[[0.99999864 0.         0.        ]\n",
      " [0.         0.99999905 0.        ]\n",
      " [0.         0.         0.99999927]]\n",
      "\n",
      "Class 7:\n",
      "Transformed Mean (mu_k): [0.4319452  0.29123022 0.6118472 ]\n",
      "Transformed Covariance (cov_k):\n",
      "[[0.99999953 0.         0.        ]\n",
      " [0.         0.99999948 0.        ]\n",
      " [0.         0.         1.00000547]]\n",
      "\n",
      "Class 8:\n",
      "Transformed Mean (mu_k): [0.13950111 0.29213835 0.3663626 ]\n",
      "Transformed Covariance (cov_k):\n",
      "[[0.99999334 0.         0.        ]\n",
      " [0.         0.99999982 0.        ]\n",
      " [0.         0.         1.00000454]]\n",
      "\n",
      "Class 9:\n",
      "Transformed Mean (mu_k): [0.45607219 0.78517243 0.19967568]\n",
      "Transformed Covariance (cov_k):\n",
      "[[1.00000198 0.         0.        ]\n",
      " [0.         1.00000546 0.        ]\n",
      " [0.         0.         0.99999959]]\n",
      "\n",
      "Class 10:\n",
      "Transformed Mean (mu_k): [0.51423626 0.59241822 0.0464486 ]\n",
      "Transformed Covariance (cov_k):\n",
      "[[0.99999588 0.         0.        ]\n",
      " [0.         0.99999692 0.        ]\n",
      " [0.         0.         0.99999952]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define KL divergence for multivariate Gaussians\n",
    "def kl_divergence(params, mu_p, cov_p, mu_q, cov_q):\n",
    "    n = mu_p.shape[0]  # Dimensionality\n",
    "    a = np.diag(params[:n])  # Diagonal scaling matrix\n",
    "    b = params[n:]  # Offset vector\n",
    "    \n",
    "    # Transformed mean and covariance\n",
    "    mu_k = a @ mu_q + b\n",
    "    cov_k = a @ cov_q @ a.T\n",
    "\n",
    "    # KL divergence components\n",
    "    term1 = np.log(np.linalg.det(cov_k) / np.linalg.det(cov_p))\n",
    "    term2 = np.trace(np.linalg.inv(cov_k) @ cov_p)\n",
    "    term3 = (mu_p - mu_k).T @ np.linalg.inv(cov_k) @ (mu_p - mu_k)\n",
    "    term4 = -n\n",
    "\n",
    "    return 0.5 * (term1 + term2 + term3 + term4)\n",
    "\n",
    "# Parameters for 10 classes of p(x) and q(x)\n",
    "classes = 10\n",
    "dim = 3  # Dimensionality of the Gaussians\n",
    "\n",
    "# Generate random means and covariances for illustration\n",
    "np.random.seed(42)\n",
    "mu_p_list = [np.random.rand(dim) for _ in range(classes)]\n",
    "cov_p_list = [np.eye(dim) for _ in range(classes)]  # Identity covariance for simplicity\n",
    "\n",
    "mu_q_list = [np.random.rand(dim) for _ in range(classes)]\n",
    "cov_q_list = [np.eye(dim) * 2 for _ in range(classes)]  # Scaled identity covariance\n",
    "\n",
    "# Placeholder for transformed Gaussian parameters\n",
    "transformed_gaussians = []\n",
    "\n",
    "for i in range(classes):\n",
    "    mu_p = mu_p_list[i]\n",
    "    cov_p = cov_p_list[i]\n",
    "    mu_q = mu_q_list[i]\n",
    "    cov_q = cov_q_list[i]\n",
    "    \n",
    "    # Initial guess for parameters\n",
    "    initial_params = np.concatenate([np.ones(dim), np.zeros(dim)])  # [a_diag, b]\n",
    "    \n",
    "    # Minimize KL divergence for the current class\n",
    "    result = minimize(kl_divergence, initial_params, args=(mu_p, cov_p, mu_q, cov_q))\n",
    "    \n",
    "    # Extract optimal parameters\n",
    "    optimal_params = result.x\n",
    "    a_optimal = np.diag(optimal_params[:dim])\n",
    "    b_optimal = optimal_params[dim:]\n",
    "    \n",
    "    # Compute transformed Gaussian parameters\n",
    "    mu_k = a_optimal @ mu_q + b_optimal\n",
    "    cov_k = a_optimal @ cov_q @ a_optimal.T\n",
    "    \n",
    "    # Save the transformed parameters\n",
    "    transformed_gaussians.append((mu_k, cov_k))\n",
    "\n",
    "# Output the transformed Gaussian distributions\n",
    "for i, (mu_k, cov_k) in enumerate(transformed_gaussians):\n",
    "    print(f\"Class {i + 1}:\")\n",
    "    print(f\"Transformed Mean (mu_k): {mu_k}\")\n",
    "    print(f\"Transformed Covariance (cov_k):\\n{cov_k}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef2b98a-e304-4029-b315-53b46fa840fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "qda = pickle.load(open('f10_qda.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63e0fc4-cd39-40c3-b7e6-155990211caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_11 = torch.load('extracted_data/X_train_11.pth')\n",
    "X_eval_11 = torch.load('extracted_data/X_eval_11.pth')\n",
    "evaldata_11 = torch.load(r\"C:\\Users\\ARITRA\\Documents\\Notebooks\\CS771_MiniProject2\\dataset\\dataset\\part_two_dataset\\eval_data\\1_eval_data.tar.pth\")\n",
    "y_test_11 = evaldata_11['targets']\n",
    "pca_train=pipeline.transform(X_train_11)\n",
    "pca_eval=pipeline.transform(X_eval_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce400385-1677-4349-b0e8-14a3ef62dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pseudo_11 = qda.predict(pca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e000b9-6c71-4dc8-9c82-fdf76df9ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example inputs\n",
    "# pca_train = np.array(...)  # n x d matrix\n",
    "# y_pseudo_11 = np.array(...)  # n x 1 vector or n-length array\n",
    "\n",
    "# Unique classes in y_pseudo_11\n",
    "classes = np.unique(y_pseudo_11)\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "means = []\n",
    "covariances = []\n",
    "\n",
    "for cls in classes:\n",
    "    # Get indices of the current class\n",
    "    indices = np.where(y_pseudo_11 == cls)[0]\n",
    "    \n",
    "    # Extract rows corresponding to the current class\n",
    "    class_data = pca_train[indices]\n",
    "    \n",
    "    # Compute mean and covariance\n",
    "    means.append(np.mean(class_data, axis=0))\n",
    "    covariances.append(np.cov(class_data, rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f671d121-d77d-4b97-a9df-38f9220146ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_p_list = qda.get_class_statistics()['means']\n",
    "cov_p_list = qda.get_class_statistics()['covariances']  # Identity covariance for simplicity\n",
    "\n",
    "mu_q_list = means\n",
    "cov_q_list = covariances  # Scaled identity covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "941eb399-3391-488a-a35b-9af514afe5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "Transformed Mean (mu_k): [ 0.72245117 -1.10639717 -0.47952426  0.53057121 -0.66278662 -0.31387892\n",
      "  0.45828145 -0.22612007  0.26091339 -0.23146773  0.54051239  0.14684358\n",
      "  0.33987582  0.25774372 -0.22241453  0.43204617 -0.55152541 -0.30305663\n",
      " -0.0772027  -0.09135301  0.26700167 -0.03132013  0.0373942   0.10497126\n",
      " -0.17761777  0.11709951 -0.18453042 -0.0618883  -0.22547649  0.26504388\n",
      "  0.08665083 -0.16907683 -0.20727861  0.05310048 -0.13724183 -0.02825839\n",
      "  0.16732125 -0.04251067  0.0107204   0.09130389  0.10061499 -0.01385897\n",
      " -0.02002014  0.02397484 -0.00154638  0.01654231  0.14574389  0.18598571\n",
      " -0.00281289 -0.02428848]\n",
      "Transformed Covariance (cov_k):\n",
      "[[ 0.60323397  0.1168835  -0.0734929  ...  0.0431948  -0.00945292\n",
      "  -0.00689636]\n",
      " [ 0.1168835   0.53403591  0.07577374 ...  0.26088746  0.00995005\n",
      "   0.10506076]\n",
      " [-0.0734929   0.07577374  0.90827066 ...  0.18386272  0.0810887\n",
      "   0.03801341]\n",
      " ...\n",
      " [ 0.0431948   0.26088746  0.18386272 ...  1.30870178 -0.02543037\n",
      "  -0.00639839]\n",
      " [-0.00945292  0.00995005  0.0810887  ... -0.02543037  1.1852534\n",
      "  -0.05419408]\n",
      " [-0.00689636  0.10506076  0.03801341 ... -0.00639839 -0.05419408\n",
      "   1.28383195]]\n",
      "\n",
      "Class 2:\n",
      "Transformed Mean (mu_k): [ 1.33137655  0.55672074 -0.21245472 -1.37533541  0.41368827 -0.39080912\n",
      " -0.20751215 -0.34403313  0.13325181  1.0310708   0.5370266  -0.16868519\n",
      " -0.4286122  -0.05189957  0.00488035  0.12397891  0.16292641  0.06633423\n",
      "  0.14532743  0.30732119  0.14219437 -0.14508089  0.04429026 -0.05315359\n",
      "  0.09733706  0.16810093  0.05088891  0.01913028 -0.09076926  0.09030115\n",
      "  0.08829302  0.104639    0.27220373 -0.02565414  0.16916052 -0.01089323\n",
      " -0.15811087  0.20272012  0.01592228  0.18751672  0.0054508  -0.09773089\n",
      "  0.11850199  0.11311022 -0.05008386  0.00179522  0.04893435 -0.16634273\n",
      "  0.0698892   0.02203713]\n",
      "Transformed Covariance (cov_k):\n",
      "[[ 0.25949413 -0.00874501 -0.03884084 ... -0.02907937  0.00991225\n",
      "  -0.05233621]\n",
      " [-0.00874501  0.35294418 -0.03532778 ... -0.15667549  0.04585188\n",
      "   0.0133309 ]\n",
      " [-0.03884084 -0.03532778  0.53187285 ...  0.005354    0.05471885\n",
      "   0.08235763]\n",
      " ...\n",
      " [-0.02907937 -0.15667549  0.005354   ...  1.01941139 -0.05976206\n",
      "  -0.00287746]\n",
      " [ 0.00991225  0.04585188  0.05471885 ... -0.05976206  1.16122277\n",
      "   0.16102622]\n",
      " [-0.05233621  0.0133309   0.08235763 ... -0.00287746  0.16102622\n",
      "   0.95102036]]\n",
      "\n",
      "Class 3:\n",
      "Transformed Mean (mu_k): [-7.79953534e-01 -5.79526888e-01 -1.63758913e-01 -3.90375595e-02\n",
      " -3.17132288e-01 -3.45507145e-01 -1.19612284e+00 -9.13805592e-01\n",
      " -3.79458619e-01  7.78983215e-02 -5.11900864e-01  4.13154224e-01\n",
      " -7.06385816e-02 -1.47912787e-01  2.75850276e-01 -4.62940170e-01\n",
      "  1.20711690e-01 -7.27066515e-01  3.64186021e-02  4.42894784e-01\n",
      "  2.09562285e-01  1.43405175e-01  1.10984906e-01 -8.87469466e-01\n",
      "  3.35543505e-02 -1.55453846e-01  1.22206853e-01  1.59635674e-01\n",
      " -1.37986284e-01 -1.43846613e-01 -7.44438435e-02  1.23448130e-02\n",
      "  1.71761012e-02 -1.71977942e-01 -5.38397210e-02 -2.10252130e-01\n",
      "  5.54241863e-02  7.16586975e-02  2.76943681e-02  3.08862179e-02\n",
      " -1.25859108e-01  9.84563403e-03 -5.39269216e-03 -2.33545624e-02\n",
      " -8.16432953e-04  4.16899555e-02 -1.14983408e-01  1.56864187e-01\n",
      "  1.77180878e-01 -2.95868188e-02]\n",
      "Transformed Covariance (cov_k):\n",
      "[[ 0.62060925 -0.04567001 -0.01026014 ... -0.07615342  0.12782536\n",
      "   0.01079281]\n",
      " [-0.04567001  0.81959622  0.36817979 ...  0.2246946   0.24631009\n",
      "   0.15505455]\n",
      " [-0.01026014  0.36817979  0.64558982 ...  0.20533962  0.12159395\n",
      "   0.08610744]\n",
      " ...\n",
      " [-0.07615342  0.2246946   0.20533962 ...  1.28380408  0.16712926\n",
      "   0.08342531]\n",
      " [ 0.12782536  0.24631009  0.12159395 ...  0.16712926  2.22871081\n",
      "  -0.12733737]\n",
      " [ 0.01079281  0.15505455  0.08610744 ...  0.08342531 -0.12733737\n",
      "   1.19555952]]\n",
      "\n",
      "Class 4:\n",
      "Transformed Mean (mu_k): [-0.44702798  0.92881673 -0.74453412  0.36429903 -0.47775507  0.17108929\n",
      "  0.02208976  0.46163098  0.41600454  0.1058487  -0.30528819  0.29410051\n",
      "  0.16627294  0.2570626   0.01286292 -0.06974389 -0.03911234  0.29656984\n",
      " -0.12819785  0.09223569 -0.10329938 -0.0162104   0.33952782 -0.04952145\n",
      "  0.12913607 -0.0100248  -0.15475394 -0.11869382 -0.19593279  0.04876493\n",
      "  0.0790834  -0.05645079  0.12208759 -0.06921396 -0.03669403  0.04265271\n",
      " -0.05858835  0.00636772 -0.0664426   0.13437259  0.03324336  0.0118928\n",
      "  0.03931039 -0.10808106  0.00907706 -0.12940743  0.03145412  0.01149323\n",
      "  0.14675117  0.11834424]\n",
      "Transformed Covariance (cov_k):\n",
      "[[ 0.62541991 -0.00328158 -0.10179373 ... -0.15207188 -0.09079064\n",
      "   0.02065267]\n",
      " [-0.00328158  0.8358504   0.09968646 ...  0.17238703  0.08348719\n",
      "  -0.05173097]\n",
      " [-0.10179373  0.09968646  1.00516079 ... -0.02100707  0.04146378\n",
      "   0.25259513]\n",
      " ...\n",
      " [-0.15207188  0.17238703 -0.02100707 ...  1.41807687  0.09005706\n",
      "   0.11691435]\n",
      " [-0.09079064  0.08348719  0.04146378 ...  0.09005706  1.69387739\n",
      "  -0.05733494]\n",
      " [ 0.02065267 -0.05173097  0.25259513 ...  0.11691435 -0.05733494\n",
      "   1.62579264]]\n",
      "\n",
      "Class 5:\n",
      "Transformed Mean (mu_k): [-0.60339424  0.03598175  1.16759649 -0.22282795 -0.60551715 -0.5838952\n",
      " -0.85977901  0.83973391  0.16897867 -0.36333397  0.52334395 -0.22292664\n",
      "  0.02905025 -0.85868361 -0.13480077  0.27278111  0.02175898  0.31609086\n",
      "  0.01927853 -0.04025172  0.22180084 -0.17752709 -0.05424693  0.4430176\n",
      " -0.06944368  0.02147583 -0.04939027  0.04085381 -0.01395588 -0.08528193\n",
      " -0.08015229 -0.14035865 -0.10952971  0.01263637 -0.08224233  0.1125234\n",
      "  0.01075427 -0.21181475  0.3267992  -0.07526702 -0.04775767 -0.09377797\n",
      "  0.07547503 -0.08800037  0.04889329 -0.19765126 -0.03764079 -0.12485338\n",
      " -0.04437002  0.0278145 ]\n",
      "Transformed Covariance (cov_k):\n",
      "[[ 0.38208607 -0.03990432  0.10267449 ... -0.09566676  0.02872106\n",
      "  -0.08029958]\n",
      " [-0.03990432  0.49420158  0.00396574 ...  0.09240281  0.10952252\n",
      "   0.04446657]\n",
      " [ 0.10267449  0.00396574  0.3317925  ... -0.17402093 -0.02478363\n",
      "   0.02409815]\n",
      " ...\n",
      " [-0.09566676  0.09240281 -0.17402093 ...  1.69794691 -0.27251347\n",
      "  -0.16136097]\n",
      " [ 0.02872106  0.10952252 -0.02478363 ... -0.27251347  1.23299823\n",
      "   0.10042732]\n",
      " [-0.08029958  0.04446657  0.02409815 ... -0.16136097  0.10042732\n",
      "   1.49808734]]\n",
      "\n",
      "Class 6:\n",
      "Transformed Mean (mu_k): [-0.14228629  1.20958884 -0.27893003  0.99747724  0.45357213  0.72824312\n",
      " -0.46220153 -0.40834581 -0.32807363 -0.26156509  0.36716115 -0.5166554\n",
      " -0.26786223 -0.1237208  -0.25957184  0.13590565 -0.23535779 -0.40150658\n",
      "  0.41563512 -0.35580574 -0.12261652 -0.1529964  -0.44184281  0.10729316\n",
      " -0.21161489 -0.29634987  0.22418879  0.28049842  0.05653296  0.13131705\n",
      "  0.1913546  -0.14324409 -0.01726297  0.0588047  -0.18311937 -0.13354591\n",
      " -0.04041731  0.21015697 -0.06585778 -0.00256993  0.00615997  0.06610612\n",
      " -0.08573027  0.2570699  -0.14347658 -0.05202168  0.25714623  0.05132431\n",
      " -0.18534482 -0.319218  ]\n",
      "Transformed Covariance (cov_k):\n",
      "[[ 0.25668444  0.00398243 -0.02159343 ... -0.09287735 -0.06284366\n",
      "  -0.05344421]\n",
      " [ 0.00398243  0.46267069 -0.05311602 ...  0.04831236 -0.06909904\n",
      "   0.3850978 ]\n",
      " [-0.02159343 -0.05311602  0.31684082 ... -0.01935474  0.07807662\n",
      "   0.05241422]\n",
      " ...\n",
      " [-0.09287735  0.04831236 -0.01935474 ...  1.6659727  -0.23428625\n",
      "   0.04336968]\n",
      " [-0.06284366 -0.06909904  0.07807662 ... -0.23428625  1.92204724\n",
      "   0.07117825]\n",
      " [-0.05344421  0.3850978   0.05241422 ...  0.04336968  0.07117825\n",
      "   2.23736692]]\n",
      "\n",
      "Class 7:\n",
      "Transformed Mean (mu_k): [-1.56186778 -0.65372553 -0.63443397 -0.82167107  0.99333127  0.03272475\n",
      "  0.75480167  0.20552124  0.24918747 -0.19684151 -0.05361814 -0.55295874\n",
      " -0.16400981  0.36499882 -0.22244883  0.31582518  0.13196366  0.03617722\n",
      "  0.1438096   0.03181148 -0.33444567  0.14928426 -0.18227237 -0.18936849\n",
      " -0.14562565  0.05998278  0.02174018 -0.21521596  0.07280918  0.14136359\n",
      " -0.06870371  0.29048152  0.03057375  0.15808202 -0.14436197  0.06168772\n",
      "  0.1557151  -0.12387169  0.09266572 -0.0684907  -0.00645748  0.01843516\n",
      "  0.04962574  0.21797456 -0.02770611 -0.07216968 -0.05974936 -0.0773468\n",
      " -0.00920076  0.07695515]\n",
      "Transformed Covariance (cov_k):\n",
      "[[ 0.57142976  0.02184429 -0.15996709 ... -0.12908531  0.20532641\n",
      "   0.06474168]\n",
      " [ 0.02184429  0.56187893  0.07765585 ...  0.04550215  0.17370756\n",
      "  -0.13750332]\n",
      " [-0.15996709  0.07765585  0.57623637 ...  0.02469764 -0.0343195\n",
      "  -0.01871427]\n",
      " ...\n",
      " [-0.12908531  0.04550215  0.02469764 ...  0.94900835 -0.00268472\n",
      "  -0.09134585]\n",
      " [ 0.20532641  0.17370756 -0.0343195  ... -0.00268472  1.74162678\n",
      "  -0.30694537]\n",
      " [ 0.06474168 -0.13750332 -0.01871427 ... -0.09134585 -0.30694537\n",
      "   1.52861814]]\n",
      "\n",
      "Class 8:\n",
      "Transformed Mean (mu_k): [-1.03690971e-01  5.02438166e-01  1.74697112e+00  7.42204722e-01\n",
      "  2.33655396e-01 -9.00561620e-01  1.06574432e+00 -2.73513881e-01\n",
      " -5.02763648e-01  4.99885782e-01 -6.09112043e-01  2.67116918e-01\n",
      "  8.17791034e-02  7.67206842e-01  2.13683239e-01  1.98029416e-01\n",
      "  8.54994131e-02 -1.26931196e-01 -6.08069596e-02 -3.90246822e-01\n",
      " -1.21421234e-01  2.66529482e-01  7.43173173e-02  8.08138465e-02\n",
      "  2.22559729e-01 -4.18245475e-02 -2.06433634e-02  1.37453533e-01\n",
      "  8.09016681e-02 -2.37325921e-01 -7.06696523e-02  2.75650358e-02\n",
      "  1.35952089e-01  6.81939317e-02  1.41768493e-03 -1.35511392e-01\n",
      "  2.10354445e-02  1.10980078e-01 -4.29369094e-02  4.07691731e-02\n",
      "  1.10566459e-01 -4.32715920e-02 -1.75067069e-03 -1.82088525e-02\n",
      "  5.32503847e-02  2.17123774e-01 -6.52209129e-03 -3.78421357e-02\n",
      " -8.22752941e-03  9.18920057e-02]\n",
      "Transformed Covariance (cov_k):\n",
      "[[ 0.29550873  0.20367376 -0.05213871 ...  0.063401   -0.076117\n",
      "  -0.0166601 ]\n",
      " [ 0.20367376  1.24128137 -0.72037803 ... -0.07372088  0.282602\n",
      "  -0.14436918]\n",
      " [-0.05213871 -0.72037803  1.79068647 ...  0.28495491 -0.07199134\n",
      "   0.07669472]\n",
      " ...\n",
      " [ 0.063401   -0.07372088  0.28495491 ...  3.3899412  -0.05587766\n",
      "  -0.57479236]\n",
      " [-0.076117    0.282602   -0.07199134 ... -0.05587766  2.38240934\n",
      "   0.20341951]\n",
      " [-0.0166601  -0.14436918  0.07669472 ... -0.57479236  0.20341951\n",
      "   2.54106096]]\n",
      "\n",
      "Class 9:\n",
      "Transformed Mean (mu_k): [ 0.74878749 -1.10889169  0.01966173  0.45313413 -0.34890819  1.29407772\n",
      "  0.13827241  0.76437489 -0.73298899  0.35397529 -0.32332355 -0.42874519\n",
      " -0.56287723 -0.1373136  -0.03162001 -0.53601243  0.74614459  0.10615904\n",
      " -0.12658357 -0.20083206 -0.15286856  0.27294334 -0.06553237  0.14943872\n",
      "  0.09064795  0.16214281  0.01122369 -0.05766278 -0.11179281 -0.1170432\n",
      " -0.03384532  0.05691811  0.06781509 -0.15793043  0.29162368  0.07305741\n",
      " -0.06169303 -0.10483457  0.08678378 -0.03660987  0.18544416  0.01958984\n",
      " -0.06032005 -0.10772652 -0.01867545  0.03645664 -0.09564177  0.04184696\n",
      " -0.01919161  0.0531504 ]\n",
      "Transformed Covariance (cov_k):\n",
      "[[ 0.31176469 -0.03038158 -0.13191939 ... -0.16060243 -0.09281941\n",
      "   0.02002535]\n",
      " [-0.03038158  0.3075383  -0.03241858 ...  0.1535449  -0.0699668\n",
      "   0.11908176]\n",
      " [-0.13191939 -0.03241858  0.55281262 ...  0.06956145  0.23243183\n",
      "  -0.00697703]\n",
      " ...\n",
      " [-0.16060243  0.1535449   0.06956145 ...  1.58776037 -0.48674595\n",
      "  -0.12118669]\n",
      " [-0.09281941 -0.0699668   0.23243183 ... -0.48674595  2.56153557\n",
      "   0.22876889]\n",
      " [ 0.02002535  0.11908176 -0.00697703 ... -0.12118669  0.22876889\n",
      "   1.11799041]]\n",
      "\n",
      "Class 10:\n",
      "Transformed Mean (mu_k): [ 1.05068695e+00  1.06428772e-01  1.79997696e-01 -5.79703401e-01\n",
      "  1.11118850e+00  4.41471436e-01  1.16893721e-03 -4.22161701e-01\n",
      "  2.13445414e-01 -1.27147489e+00 -3.08068753e-01  5.58668112e-01\n",
      "  9.42926149e-01 -3.86339321e-01  3.46967179e-01 -1.51515559e-01\n",
      "  2.65215210e-01  3.34809114e-01 -3.28265426e-01  9.11141945e-02\n",
      "  2.00858363e-02 -1.33309368e-01  5.18740719e-02  7.12946205e-02\n",
      "  2.61994434e-02 -9.99686587e-03  1.77534105e-02 -2.68781514e-01\n",
      "  2.75344616e-01  9.67154386e-03  5.15172165e-02  1.79653169e-02\n",
      " -1.02924584e-01 -1.31143054e-01  7.77601932e-02  6.66210564e-02\n",
      "  2.23418999e-02 -6.23067003e-02 -1.04352200e-01 -2.51309798e-01\n",
      " -2.08830146e-01  2.65332834e-02 -4.74810051e-02 -2.25768079e-01\n",
      "  1.39114677e-02  1.47054090e-01  2.68956670e-02  8.15058410e-02\n",
      " -5.41099885e-02 -1.00118598e-01]\n",
      "Transformed Covariance (cov_k):\n",
      "[[ 0.3263771   0.03884789 -0.06594437 ... -0.08910844  0.07317872\n",
      "  -0.05578027]\n",
      " [ 0.03884789  0.41947916 -0.09441608 ... -0.03282588 -0.02422324\n",
      "   0.049135  ]\n",
      " [-0.06594437 -0.09441608  0.37678117 ...  0.11644745  0.00949481\n",
      "  -0.1283928 ]\n",
      " ...\n",
      " [-0.08910844 -0.03282588  0.11644745 ...  1.77022685  0.18763478\n",
      "   0.02268519]\n",
      " [ 0.07317872 -0.02422324  0.00949481 ...  0.18763478  1.08579652\n",
      "   0.35035135]\n",
      " [-0.05578027  0.049135   -0.1283928  ...  0.02268519  0.35035135\n",
      "   1.72969312]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for transformed Gaussian parameters\n",
    "classes = 10\n",
    "dim = 50\n",
    "transformed_gaussians = []\n",
    "\n",
    "for i in range(classes):\n",
    "    mu_p = mu_p_list[i]\n",
    "    cov_p = cov_p_list[i]\n",
    "    mu_q = mu_q_list[i]\n",
    "    cov_q = cov_q_list[i]\n",
    "    \n",
    "    # Initial guess for parameters\n",
    "    initial_params = np.concatenate([np.ones(dim), np.zeros(dim)])  # [a_diag, b]\n",
    "    \n",
    "    # Minimize KL divergence for the current class\n",
    "    result = minimize(kl_divergence, initial_params, args=(mu_p, cov_p, mu_q, cov_q))\n",
    "    \n",
    "    # Extract optimal parameters\n",
    "    optimal_params = result.x\n",
    "    a_optimal = np.diag(optimal_params[:dim])\n",
    "    b_optimal = optimal_params[dim:]\n",
    "    \n",
    "    # Compute transformed Gaussian parameters\n",
    "    mu_k = a_optimal @ mu_q + b_optimal\n",
    "    cov_k = a_optimal @ cov_q @ a_optimal.T\n",
    "    \n",
    "    # Save the transformed parameters\n",
    "    transformed_gaussians.append((mu_k, cov_k))\n",
    "\n",
    "# Output the transformed Gaussian distributions\n",
    "for i, (mu_k, cov_k) in enumerate(transformed_gaussians):\n",
    "    print(f\"Class {i + 1}:\")\n",
    "    print(f\"Transformed Mean (mu_k): {mu_k}\")\n",
    "    print(f\"Transformed Covariance (cov_k):\\n{cov_k}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06cfa8e8-d486-4e1c-95c9-f655b7d42c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the transformed data points\n",
    "transformed_pca_data = []\n",
    "\n",
    "# Loop through each data point in pca_train\n",
    "for idx, data_point in enumerate(pca_train):\n",
    "    # Look up the class of the current data point\n",
    "    class_label = y_pseudo_11[idx]\n",
    "    \n",
    "    # Retrieve optimal transformation parameters for the identified class\n",
    "    a_optimal = np.diag(np.sqrt(np.diag(transformed_gaussians[class_label][1]) / np.diag(cov_q_list[class_label])))  # Diagonal scaling matrix\n",
    "    b_optimal = transformed_gaussians[class_label][0] - a_optimal @ mu_q_list[class_label]  # Shift vector to align the means\n",
    "\n",
    "    # Transform the current data point\n",
    "    transformed_data_point = (a_optimal @ data_point) + b_optimal\n",
    "\n",
    "    # Append the transformed data point to the list\n",
    "    transformed_pca_data.append(transformed_data_point)\n",
    "\n",
    "# Convert the list to a NumPy array for further processing\n",
    "transformed_pca_data = np.array(transformed_pca_data)\n",
    "\n",
    "# Output the transformed data points for verification (optional)\n",
    "# print(f\"Transformed PCA Data (First 5 rows):\\n{transformed_pca_data[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd90f674-3fc8-4cd6-abd6-a07e4e4111d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qda.update(transformed_pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d04d87f-2022-4d20-b3a4-fa45ebd4a293",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QDAClassifier' object has no attribute 'predict_modified'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prediction\u001b[38;5;241m=\u001b[39mqda\u001b[38;5;241m.\u001b[39mpredict_modified(pca_eval)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(prediction \u001b[38;5;241m==\u001b[39m y_test_11)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'QDAClassifier' object has no attribute 'predict_modified'"
     ]
    }
   ],
   "source": [
    "prediction=qda.predict_modified(pca_eval)\n",
    "print(np.mean(prediction == y_test_11)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963c88c-38b1-493a-9dc2-8c4967c24adf",
   "metadata": {},
   "source": [
    "Accuracy is not upto the mark, though the approach was interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b1cbe-e515-41eb-812a-fb4ba32563b4",
   "metadata": {},
   "source": [
    "## 3) Loss function minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4238037-e159-4b05-ae30-6ad35248733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "\n",
    "# Paths to train and eval datasets\n",
    "train_files = sorted(glob.glob(\"X_train_*.pth\"))  # Glob pattern for train datasets\n",
    "eval_files = sorted(glob.glob(\"X_eval_*.pth\"))    # Glob pattern for eval datasets\n",
    "\n",
    "# Load all train datasets\n",
    "train_datasets = [torch.load(file) for file in train_files]\n",
    "\n",
    "# Load all eval datasets\n",
    "eval_datasets = [torch.load(file) for file in eval_files]\n",
    "\n",
    "# Store datasets in a dictionary (optional)\n",
    "datasets = {\n",
    "    \"train\": train_datasets,\n",
    "    \"eval\": eval_datasets\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(f\"Loaded {len(train_datasets)} train datasets.\")\n",
    "print(f\"Loaded {len(eval_datasets)} eval datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a2e52-01b1-4826-b168-62ba0c3ec8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval_datasets = []\n",
    "for i in range(1, 11):\n",
    "    y_eval_datasets.append(torch.load(f\"dataset\\\\dataset\\\\part_two_dataset\\\\eval_data\\\\{i}_eval_data.tar.pth\")['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30a9ea-571d-4555-b16e-3edba447a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def compute_pseudo_labels(X, class_means):\n",
    "    \"\"\"\n",
    "    Computes pseudo-labels for the dataset using Euclidean distance.\n",
    "    \"\"\"\n",
    "    # Ensure X is a tensor with consistent dtype\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "    elif X.dtype != torch.float32:\n",
    "        X = X.to(torch.float32)\n",
    "    \n",
    "    # Convert class_means dict to a tensor with consistent dtype\n",
    "    class_means_tensor = torch.stack(\n",
    "        [torch.tensor(class_means[k], dtype=torch.float32) for k in sorted(class_means.keys())]\n",
    "    )  # Shape: [K, d]\n",
    "    \n",
    "    # Calculate the Euclidean distance between each data point and each class mean\n",
    "    distances = torch.cdist(X, class_means_tensor)  # Shape: [N, K]\n",
    "    \n",
    "    # Get the index of the class with the minimum distance for each data point\n",
    "    pseudo_labels = torch.argmin(distances, dim=1)  # Shape: [N]\n",
    "    \n",
    "    return pseudo_labels\n",
    "\n",
    "import torch\n",
    "\n",
    "# Example data\n",
    "X = train_datasets_pre_processed[10]  # `data` is a tensor of size (2500, 2048)\n",
    "pseudo_labels = compute_pseudo_labels(train_datasets_pre_processed[10], class_means)  # A tensor of size (2500,)\n",
    "# `class_means` is now a list of tensors: [tensor(2048), tensor(2048), ..., tensor(2048)]\n",
    "\n",
    "# Initialize `updated_means` as a list of tensors similar to `class_means`\n",
    "updated_means = [torch.tensor(class_mean).clone().detach().zero_().requires_grad_(True) for mean, class_mean in class_means.items()]\n",
    "\n",
    "def loss_function(X, updated_means, class_means, pseudo_labels, lambda_reg=0.1):\n",
    "    \"\"\"\n",
    "    Optimized loss function calculation with soft assignment and regularization of class means.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: torch.Tensor of shape (N, d), dataset with N samples and d features.\n",
    "    - updated_means: list of tensors, updated class means, each of shape (d,).\n",
    "    - class_means: list of tensors, original class means, each of shape (d,).\n",
    "    - pseudo_labels: torch.Tensor of shape (N,), pseudo-labels for each sample.\n",
    "    - lambda_reg: Scalar, the weight of the regularization term (default is 0.1).\n",
    "    \n",
    "    Returns:\n",
    "    - loss: Scalar torch.Tensor, the computed loss value.\n",
    "    \"\"\"\n",
    "    # Convert class_means and updated_means to tensors\n",
    "    updated_means_tensor = torch.stack(updated_means)  # Shape: [K, d]\n",
    "    class_means_tensor = torch.stack([torch.tensor(class_mean).clone().detach().zero_().requires_grad_(True) for mean, class_mean in class_means.items()])  # Shape: [K, d]\n",
    "    \n",
    "    # Compute distances between all samples and all class means\n",
    "    X_expanded = torch.tensor(X).unsqueeze(1)  # Shape: [N, 1, d]\n",
    "    updated_distances = torch.norm(X_expanded - updated_means_tensor, dim=2)  # Shape: [N, K]\n",
    "    class_distances = torch.norm(X_expanded - class_means_tensor, dim=2)  # Shape: [N, K]\n",
    "    \n",
    "    # Soft assignment (probabilities for each class based on the distances)\n",
    "    updated_soft_assignment = torch.softmax(-updated_distances, dim=1)  # Shape: [N, K]\n",
    "    class_soft_assignment = torch.softmax(-class_distances, dim=1)  # Shape: [N, K]\n",
    "    \n",
    "    # First term: log(num / den) for updated_means\n",
    "    updated_num = torch.gather(updated_distances**2, 1, pseudo_labels.unsqueeze(1)).squeeze(1)  # Shape: [N]\n",
    "    updated_den = (updated_distances**2).sum(dim=1)  # Shape: [N]\n",
    "    first_term = -torch.log(updated_num / updated_den).sum()\n",
    "\n",
    "    # Second term: log(num / den) for combined distances\n",
    "    combined_num = updated_num + torch.gather(class_distances**2, 1, pseudo_labels.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # Compute the denominator for the second term\n",
    "    combined_den = (updated_distances**2).sum(dim=1) + (class_distances**2).sum(dim=1)\n",
    "    \n",
    "    # Compute cross-sample pairwise distances\n",
    "    pairwise_distances = torch.cdist(torch.tensor(X), torch.tensor(X), p=2)  # Shape: [N, N]\n",
    "    mask = (pseudo_labels.unsqueeze(1) != pseudo_labels.unsqueeze(0))  # Mask for different class pairs\n",
    "    combined_den += (pairwise_distances**2 * mask).sum(dim=1)\n",
    "    \n",
    "    second_term = -torch.log(combined_num / combined_den).sum()\n",
    "\n",
    "    # Regularization term to keep the means as far apart as possible\n",
    "    pairwise_means_distances = torch.cdist(updated_means_tensor, updated_means_tensor, p=2)  # Shape: [K, K]\n",
    "    regularization_term = torch.sum(1.0 / (pairwise_means_distances + 1e-6))  # Avoid division by zero\n",
    "\n",
    "    # Total loss with regularization\n",
    "    loss = -first_term - second_term - lambda_reg * regularization_term  # Subtract regularization to maximize distance\n",
    "    return loss\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(updated_means, lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100  # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    loss = loss_function(X, updated_means, class_means, pseudo_labels)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update `updated_means`\n",
    "\n",
    "    # Normalize updated_means after each step (optional)\n",
    "    for i in range(len(updated_means)):\n",
    "        updated_means[i].data = updated_means[i].data / updated_means[i].norm()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86d20a-720b-4f14-8251-ab67d770f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, updated_means):\n",
    "    \"\"\"\n",
    "    Predicts class labels for the given data points using the updated means.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: torch.Tensor of shape (N, d), where N is the number of data points and d is the feature dimension.\n",
    "    - updated_means: list of torch.Tensor, where each tensor is the mean vector of a class (d-dimensional).\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: torch.Tensor of shape (N,), the predicted class labels.\n",
    "    \"\"\"\n",
    "    # Convert updated_means to a tensor of shape (K, d)\n",
    "    updated_means_tensor = torch.stack(updated_means)  # Shape: [K, d]\n",
    "    \n",
    "    # Compute distances between each data point and each class mean\n",
    "    distances = torch.cdist(torch.tensor(X), updated_means_tensor)  # Shape: [N, K]\n",
    "    \n",
    "    # Predict the class with the minimum distance\n",
    "    predictions = torch.argmin(distances, dim=1)  # Shape: [N]\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(X_eval, y_eval, updated_means):\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = predict(X_eval, updated_means)\n",
    "    \n",
    "    # Compare predictions with true labels\n",
    "    correct = (predictions == y_eval).sum().item()\n",
    "    total = y_eval.shape[0]  # Use .shape instead of .size\n",
    "    print(predictions)\n",
    "    # Compute accuracy\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluate(eval_datasets_pre_processed[10], y_eval_datasets[1], updated_means)\n",
    "print(f\"Accuracy on evaluation data: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
